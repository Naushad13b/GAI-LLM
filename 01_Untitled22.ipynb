{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c334b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing target column: Islamabad_PM2.5\n",
      "\n",
      "Training and evaluating model: proposed4\n",
      "Epoch 1/200\n",
      "368/368 [==============================] - 22s 25ms/step - loss: 0.0129 - val_loss: 2.7061e-04 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 9.0328e-04 - val_loss: 3.9226e-04 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "365/368 [============================>.] - ETA: 0s - loss: 7.8964e-04\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 7.8939e-04 - val_loss: 2.7710e-04 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 5.9002e-04 - val_loss: 3.3351e-04 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 5.9178e-04 - val_loss: 9.8958e-05 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 5.5096e-04 - val_loss: 4.5174e-05 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "368/368 [==============================] - ETA: 0s - loss: 5.2802e-04\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 5.2802e-04 - val_loss: 6.2267e-05 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 4.4493e-04 - val_loss: 3.3182e-05 - lr: 2.5000e-04\n",
      "Epoch 9/200\n",
      "367/368 [============================>.] - ETA: 0s - loss: 4.3161e-04\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 4.3153e-04 - val_loss: 4.2023e-05 - lr: 2.5000e-04\n",
      "Epoch 10/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 4.0678e-04 - val_loss: 2.7886e-05 - lr: 1.2500e-04\n",
      "Epoch 11/200\n",
      "366/368 [============================>.] - ETA: 0s - loss: 3.8454e-04\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.8532e-04 - val_loss: 4.0060e-05 - lr: 1.2500e-04\n",
      "Epoch 12/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.8038e-04 - val_loss: 1.6458e-05 - lr: 6.2500e-05\n",
      "Epoch 13/200\n",
      "365/368 [============================>.] - ETA: 0s - loss: 3.7159e-04\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.7201e-04 - val_loss: 2.6766e-05 - lr: 6.2500e-05\n",
      "Epoch 14/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.7474e-04 - val_loss: 1.5719e-05 - lr: 3.1250e-05\n",
      "Epoch 15/200\n",
      "368/368 [==============================] - ETA: 0s - loss: 3.6333e-04\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.6333e-04 - val_loss: 2.8274e-05 - lr: 3.1250e-05\n",
      "Epoch 16/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.6052e-04 - val_loss: 1.4600e-05 - lr: 1.5625e-05\n",
      "Epoch 17/200\n",
      "366/368 [============================>.] - ETA: 0s - loss: 3.6137e-04\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.6127e-04 - val_loss: 1.5589e-05 - lr: 1.5625e-05\n",
      "Epoch 18/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.6734e-04 - val_loss: 1.3553e-05 - lr: 7.8125e-06\n",
      "Epoch 19/200\n",
      "365/368 [============================>.] - ETA: 0s - loss: 3.6775e-04\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.6777e-04 - val_loss: 1.5546e-05 - lr: 7.8125e-06\n",
      "Epoch 20/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.4848e-04 - val_loss: 1.4381e-05 - lr: 3.9063e-06\n",
      "Epoch 21/200\n",
      "367/368 [============================>.] - ETA: 0s - loss: 3.4608e-04\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.4602e-04 - val_loss: 1.3500e-05 - lr: 3.9063e-06\n",
      "Epoch 22/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.4969e-04 - val_loss: 1.2972e-05 - lr: 1.9531e-06\n",
      "Epoch 23/200\n",
      "367/368 [============================>.] - ETA: 0s - loss: 3.5330e-04\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.5329e-04 - val_loss: 1.3331e-05 - lr: 1.9531e-06\n",
      "Epoch 24/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.5671e-04 - val_loss: 1.3235e-05 - lr: 9.7656e-07\n",
      "Epoch 25/200\n",
      "366/368 [============================>.] - ETA: 0s - loss: 3.4694e-04\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.4712e-04 - val_loss: 1.3341e-05 - lr: 9.7656e-07\n",
      "Epoch 26/200\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.4402e-04 - val_loss: 1.3525e-05 - lr: 4.8828e-07\n",
      "Epoch 27/200\n",
      "365/368 [============================>.] - ETA: 0s - loss: 3.4632e-04\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 3.4647e-04 - val_loss: 1.3515e-05 - lr: 4.8828e-07\n",
      "Epoch 27: early stopping\n",
      "153/153 [==============================] - 4s 6ms/step\n",
      "  3328/106575 [..............................] - ETA: 132:34:42"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10268\\3548849820.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;31m# Process and save results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mprocess_and_save_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10268\\3548849820.py\u001b[0m in \u001b[0;36mprocess_and_save_results\u001b[1;34m(model_name, column, df, history, y_test_inverse, y_pred_inverse, model, X_train, X_test, feature_transformed)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mforecast_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeseriesGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[0mpredicted_values_forecast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforecast_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[0mpredicted_values_forecast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_values_forecast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2653\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2654\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2655\u001b[1;33m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2656\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[0;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m   )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1322\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0mflat_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, LSTM, Bidirectional, SimpleRNN, GRU, Input, Flatten, Multiply\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "# Enable GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Load data\n",
    "files = glob.glob(r'D:\\A_NAUSHAD\\E\\Dataset\\Book7.csv')\n",
    "data = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n",
    "\n",
    "# Time column to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Feature and target columns\n",
    "feature_columns = ['Islamabad_PM2.5', 'Dhaka_PM2.5', 'Beijing_PM2.5', 'Delhi_PM2.5']  # Example feature columns\n",
    "target_columns = feature_columns  # Use feature_columns as target_columns for prediction\n",
    "\n",
    "def create_model(input_shape):\n",
    "    models = {}\n",
    "    \n",
    "#     # LSTM Model\n",
    "#     model_LSTM = Sequential()\n",
    "#     model_LSTM.add(LSTM(16, activation='relu', input_shape=input_shape))\n",
    "#     model_LSTM.add(Dropout(0.25))\n",
    "#     model_LSTM.add(Dense(1))\n",
    "#     model_LSTM.compile(optimizer='adam', loss='mse')\n",
    "#     models['LSTM'] = model_LSTM\n",
    "    \n",
    "#     # GRU Model\n",
    "#     model_GRU = Sequential()\n",
    "#     model_GRU.add(GRU(16, activation='tanh', input_shape=input_shape))\n",
    "#     model_GRU.add(Dropout(0.25))\n",
    "#     model_GRU.add(Dense(1))\n",
    "#     model_GRU.compile(optimizer='adam', loss='mse')\n",
    "#     models['GRU'] = model_GRU\n",
    "    \n",
    "#     # RNN Model\n",
    "#     model_RNN = Sequential()\n",
    "#     model_RNN.add(SimpleRNN(16, activation='tanh', input_shape=input_shape))\n",
    "#     model_RNN.add(Dropout(0.25))\n",
    "#     model_RNN.add(Dense(1))\n",
    "#     model_RNN.compile(optimizer='adam', loss='mse')\n",
    "#     models['RNN'] = model_RNN\n",
    "    \n",
    "#     # BiLSTM Model\n",
    "#     model_BiLSTM = Sequential()\n",
    "#     model_BiLSTM.add(Bidirectional(LSTM(8, activation='tanh'), input_shape=input_shape))\n",
    "#     model_BiLSTM.add(Dropout(0.25))\n",
    "#     model_BiLSTM.add(Flatten())\n",
    "#     model_BiLSTM.add(Dense(1))\n",
    "#     model_BiLSTM.compile(optimizer='adam', loss='mse')\n",
    "#     models['BiLSTM'] = model_BiLSTM\n",
    "    \n",
    "#     # CNN Model\n",
    "#     model_CNN = Sequential()\n",
    "#     model_CNN.add(Conv1D(filters=16, kernel_size=1, input_shape=input_shape))\n",
    "#     model_CNN.add(Flatten())\n",
    "#     model_CNN.add(Dropout(0.25))\n",
    "#     model_CNN.add(Dense(1))\n",
    "#     model_CNN.compile(optimizer='adam', loss='mse')\n",
    "#     models['CNN'] = model_CNN\n",
    "    \n",
    "#     # Proposed Model 1\n",
    "#     model_proposed1 = Sequential()\n",
    "#     model_proposed1.add(Bidirectional(LSTM(64, activation='tanh', return_sequences=True), input_shape=input_shape))\n",
    "#     model_proposed1.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "#     model_proposed1.add(Bidirectional(LSTM(16, activation='tanh', return_sequences=False)))\n",
    "#     model_proposed1.add(Dropout(0.1))\n",
    "#     model_proposed1.add(Dense(1))\n",
    "#     model_proposed1.compile(optimizer='adam', loss='mse')\n",
    "#     models['proposed1'] = model_proposed1\n",
    "    \n",
    "#     # Proposed Model 2\n",
    "#     model_proposed2 = Sequential()\n",
    "#     model_proposed2.add(Conv1D(filters=16, kernel_size=3, padding='causal', activation='relu', input_shape=input_shape))\n",
    "#     model_proposed2.add(Bidirectional(LSTM(64, activation='tanh', return_sequences=True)))\n",
    "#     model_proposed2.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "#     model_proposed2.add(Bidirectional(LSTM(16, activation='tanh', return_sequences=False)))\n",
    "#     model_proposed2.add(Dropout(0.1))\n",
    "#     model_proposed2.add(Dense(1))\n",
    "#     model_proposed2.compile(optimizer='adam', loss='mse')\n",
    "#     models['proposed2'] = model_proposed2\n",
    "    \n",
    "#     # Proposed Model 3\n",
    "#     model_proposed3 = Sequential()\n",
    "#     model_proposed3.add(Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#     model_proposed3.add(Bidirectional(LSTM(64, activation='tanh', return_sequences=True)))\n",
    "#     model_proposed3.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "#     model_proposed3.add(Bidirectional(LSTM(16, activation='tanh', return_sequences=False)))\n",
    "#     model_proposed3.add(Dropout(0.1))\n",
    "#     model_proposed3.add(Dense(1))\n",
    "#     model_proposed3.compile(optimizer='adam', loss='mse')\n",
    "#     models['proposed3'] = model_proposed3\n",
    "\n",
    "    # Proposed Model 4 (WaveNet with LSTM)\n",
    "    def wavenet_layer(inputs, dilation_rate=2, kernel_size=2):\n",
    "        dilated_conv = Conv1D(filters=32, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(inputs)\n",
    "        gated_activation = Activation('tanh')(dilated_conv)\n",
    "        gated_activation = Multiply()([gated_activation, Activation('sigmoid')(dilated_conv)])\n",
    "        return gated_activation\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = wavenet_layer(inputs)\n",
    "    x = wavenet_layer(x, dilation_rate=4)\n",
    "    x = wavenet_layer(x, dilation_rate=8)\n",
    "    x = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(16, activation='tanh', return_sequences=False))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model_proposed4 = Model(inputs=inputs, outputs=outputs)\n",
    "    model_proposed4.compile(optimizer='adam', loss='mse')\n",
    "    models['proposed4'] = model_proposed4\n",
    "\n",
    "    # Proposed Model 5 (WaveNet)\n",
    "    def WaveNet(input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "        # Add more layers as per your WaveNet architecture\n",
    "        outputs = Conv1D(filters=1, kernel_size=1, activation='linear')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    wavenet_input_shape = (X_train.shape[1], X_train.shape[2]) if len(X_train.shape) == 3 else (X_train.shape[1], 1)\n",
    "    wavenet_model = WaveNet(wavenet_input_shape)\n",
    "    models['proposed5'] = wavenet_model\n",
    "\n",
    "    return models\n",
    "\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mslr = tf.keras.metrics.mean_squared_logarithmic_error(y_true, y_pred)\n",
    "\n",
    "    results = {\n",
    "        \"mae\": mae.numpy().mean(),\n",
    "        \"mse\": mse.numpy().mean(),\n",
    "        \"rmse\": rmse.numpy().mean(),\n",
    "        \"mape\": mape.numpy().mean(),\n",
    "        \"mslr\": mslr.numpy().mean(),\n",
    "    }\n",
    "    \n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def process_and_save_results(model_name, column, df, history, y_test_inverse, y_pred_inverse, model, X_train, X_test, feature_transformed):\n",
    "    # Forecasting dates\n",
    "    forecast_dates = pd.date_range(start='2023-01-28', end='2024-12-31', freq='H')\n",
    "    df_forecast = pd.DataFrame(index=forecast_dates)\n",
    "    \n",
    "    # Scale and reshape features\n",
    "    feature_scaled = feature_transformed.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_scaled = scaler.fit_transform(feature_scaled)\n",
    "\n",
    "    forecast_generator = TimeseriesGenerator(feature_scaled, np.zeros(len(feature_scaled)), length=len(X_train), sampling_rate=1, batch_size=1)\n",
    "    predicted_values_forecast = model.predict(forecast_generator)\n",
    "    predicted_values_forecast = scaler.inverse_transform(predicted_values_forecast)\n",
    "\n",
    "    if len(predicted_values_forecast) > len(df_forecast):\n",
    "        predicted_values_forecast = predicted_values_forecast[:len(df_forecast)]\n",
    "    else:\n",
    "        forecast_values = np.full((len(df_forecast), 1), np.nan)\n",
    "        forecast_values[:len(predicted_values_forecast)] = predicted_values_forecast\n",
    "        predicted_values_forecast = forecast_values\n",
    "\n",
    "    df_forecast[column] = predicted_values_forecast\n",
    "    save_to_csv(df_forecast, f'D:/A_NAUSHAD/E/RESULTS/FORE/{model_name}_{column}_Wave_fore.csv')\n",
    "\n",
    "    # Plot forecast\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_forecast.index, df_forecast[column], label='Forecasted')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.title(f'Forecast of Hourly {column} concentration using {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save loss history\n",
    "    save_to_csv(pd.DataFrame(history.history['loss']), f'D:/A_NAUSHAD/E/RESULTS/LOSS/{model_name}_{column}_Wave_loss.csv')\n",
    "    save_to_csv(pd.DataFrame(history.history['val_loss']), f'D:/A_NAUSHAD/E/RESULTS/LOSS/{model_name}_{column}_Wave_val_loss.csv')\n",
    "\n",
    "    # Save training and testing predictions\n",
    "    predictions_train = model.predict(X_train)\n",
    "    save_to_csv(pd.DataFrame(predictions_train), f'D:/A_NAUSHAD/E/RESULTS/PRED/{model_name}_{column}_Wave_train_pred.csv')\n",
    "    \n",
    "    predictions_test = model.predict(X_test)\n",
    "    save_to_csv(pd.DataFrame(predictions_test), f'D:/A_NAUSHAD/E/RESULTS/PRED/{model_name}_{column}_Wave_test_pred.csv')\n",
    "\n",
    "    # Save evaluation results\n",
    "    eval_results = evaluate_preds(y_true=y_test_inverse, y_pred=y_pred_inverse)\n",
    "    eval_df = pd.DataFrame.from_dict(eval_results, orient='index', columns=['value'])\n",
    "    save_to_csv(eval_df, f'D:/A_NAUSHAD/E/RESULTS/EVAL/{model_name}_{column}_Wave_eval.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "lr_monitor = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    factor=0.5,\n",
    "    cooldown=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Preprocess features and target\n",
    "for target_column in feature_columns:\n",
    "    print(f\"\\nProcessing target column: {target_column}\")\n",
    "    \n",
    "    # Prepare target data\n",
    "    target = data[target_column].values\n",
    "    scaler_target = MinMaxScaler()\n",
    "    target_scaled = scaler_target.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "    # Prepare features\n",
    "    features = data[feature_columns].values\n",
    "    scaler_features = MinMaxScaler()\n",
    "    features_scaled = scaler_features.fit_transform(features)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_scaled, test_size=0.15, random_state=1, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=1, shuffle=False)\n",
    "    \n",
    "    # Reshape data for LSTM/GRU input\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Create models\n",
    "    models = create_model((X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining and evaluating model: {model_name}\")\n",
    "        \n",
    "        # Train model with callbacks\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=200,\n",
    "            batch_size=64,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "            callbacks=[lr_monitor, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_test_inverse = scaler_target.inverse_transform(y_test)\n",
    "        y_pred_inverse = scaler_target.inverse_transform(y_pred)\n",
    "        \n",
    "        # Process and save results\n",
    "        process_and_save_results(model_name, target_column, data, history, y_test_inverse, y_pred_inverse, model, X_train, X_test, features_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b28e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
