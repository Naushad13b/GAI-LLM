{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a4eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "Processing target column: Islamabad_PM2.5\n",
      "WARNING:tensorflow:From C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "\n",
      "Training and evaluating model: LSTM\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "368/368 [==============================] - 4s 6ms/step - loss: 0.0516 - val_loss: 0.0212 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0218 - val_loss: 0.0140 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0140 - val_loss: 0.0091 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "368/368 [==============================] - 2s 5ms/step - loss: 0.0097 - val_loss: 0.0053 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0071 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "368/368 [==============================] - 1s 4ms/step - loss: 0.0057 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0047 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "368/368 [==============================] - 1s 4ms/step - loss: 0.0039 - val_loss: 6.0423e-04 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "368/368 [==============================] - 1s 4ms/step - loss: 0.0036 - val_loss: 6.0489e-04 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0034 - val_loss: 3.7108e-04 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0032 - val_loss: 3.5590e-04 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "362/368 [============================>.] - ETA: 0s - loss: 0.0031\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0031 - val_loss: 3.2145e-04 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "368/368 [==============================] - 2s 4ms/step - loss: 0.0029 - val_loss: 2.2616e-04 - lr: 5.0000e-04\n",
      "Epoch 14/200\n",
      "368/368 [==============================] - 2s 5ms/step - loss: 0.0029 - val_loss: 2.0685e-04 - lr: 5.0000e-04\n",
      "Epoch 15/200\n",
      "359/368 [============================>.] - ETA: 0s - loss: 0.0027\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "368/368 [==============================] - 2s 5ms/step - loss: 0.0027 - val_loss: 2.9301e-04 - lr: 5.0000e-04\n",
      "Epoch 16/200\n",
      "368/368 [==============================] - 2s 5ms/step - loss: 0.0027 - val_loss: 2.2209e-04 - lr: 2.5000e-04\n",
      "Epoch 17/200\n",
      "367/368 [============================>.] - ETA: 0s - loss: 0.0028\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "368/368 [==============================] - 2s 5ms/step - loss: 0.0028 - val_loss: 2.4650e-04 - lr: 2.5000e-04\n",
      "Epoch 18/200\n",
      "368/368 [==============================] - 2s 5ms/step - loss: 0.0027 - val_loss: 2.4029e-04 - lr: 1.2500e-04\n",
      "Epoch 19/200\n",
      "368/368 [==============================] - ETA: 0s - loss: 0.0027\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "368/368 [==============================] - 2s 5ms/step - loss: 0.0027 - val_loss: 2.2463e-04 - lr: 1.2500e-04\n",
      "Epoch 19: early stopping\n",
      "153/153 [==============================] - 1s 2ms/step\n",
      " 17392/106575 [===>..........................] - ETA: 32:03:00"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, LSTM, Bidirectional, SimpleRNN, GRU, Input, Flatten, Multiply\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "# Enable GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Load data\n",
    "files = glob.glob(r'D:\\A_NAUSHAD\\E\\Dataset\\Book7.csv')\n",
    "data = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n",
    "\n",
    "# Time column to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Feature and target columns\n",
    "feature_columns = ['Islamabad_PM2.5', 'Dhaka_PM2.5', 'Beijing_PM2.5', 'Delhi_PM2.5']  # Example feature columns\n",
    "target_columns = feature_columns  # Use feature_columns as target_columns for prediction\n",
    "\n",
    "def create_model(input_shape):\n",
    "    models = {}\n",
    "    \n",
    "    # LSTM Model\n",
    "    model_LSTM = Sequential()\n",
    "    model_LSTM.add(LSTM(16, activation='relu', input_shape=input_shape))\n",
    "    model_LSTM.add(Dropout(0.25))\n",
    "    model_LSTM.add(Dense(1))\n",
    "    model_LSTM.compile(optimizer='adam', loss='mse')\n",
    "    models['LSTM'] = model_LSTM\n",
    "    \n",
    "    # GRU Model\n",
    "    model_GRU = Sequential()\n",
    "    model_GRU.add(GRU(16, activation='tanh', input_shape=input_shape))\n",
    "    model_GRU.add(Dropout(0.25))\n",
    "    model_GRU.add(Dense(1))\n",
    "    model_GRU.compile(optimizer='adam', loss='mse')\n",
    "    models['GRU'] = model_GRU\n",
    "    \n",
    "    # RNN Model\n",
    "    model_RNN = Sequential()\n",
    "    model_RNN.add(SimpleRNN(16, activation='tanh', input_shape=input_shape))\n",
    "    model_RNN.add(Dropout(0.25))\n",
    "    model_RNN.add(Dense(1))\n",
    "    model_RNN.compile(optimizer='adam', loss='mse')\n",
    "    models['RNN'] = model_RNN\n",
    "    \n",
    "    # BiLSTM Model\n",
    "    model_BiLSTM = Sequential()\n",
    "    model_BiLSTM.add(Bidirectional(LSTM(8, activation='tanh'), input_shape=input_shape))\n",
    "    model_BiLSTM.add(Dropout(0.25))\n",
    "    model_BiLSTM.add(Flatten())\n",
    "    model_BiLSTM.add(Dense(1))\n",
    "    model_BiLSTM.compile(optimizer='adam', loss='mse')\n",
    "    models['BiLSTM'] = model_BiLSTM\n",
    "    \n",
    "    # CNN Model\n",
    "    model_CNN = Sequential()\n",
    "    model_CNN.add(Conv1D(filters=16, kernel_size=1, input_shape=input_shape))\n",
    "    model_CNN.add(Flatten())\n",
    "    model_CNN.add(Dropout(0.25))\n",
    "    model_CNN.add(Dense(1))\n",
    "    model_CNN.compile(optimizer='adam', loss='mse')\n",
    "    models['CNN'] = model_CNN\n",
    "    \n",
    "    # Proposed Model 1\n",
    "    model_proposed1 = Sequential()\n",
    "    model_proposed1.add(Bidirectional(LSTM(64, activation='tanh', return_sequences=True), input_shape=input_shape))\n",
    "    model_proposed1.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "    model_proposed1.add(Bidirectional(LSTM(16, activation='tanh', return_sequences=False)))\n",
    "    model_proposed1.add(Dropout(0.1))\n",
    "    model_proposed1.add(Dense(1))\n",
    "    model_proposed1.compile(optimizer='adam', loss='mse')\n",
    "    models['proposed1'] = model_proposed1\n",
    "    \n",
    "    # Proposed Model 2\n",
    "    model_proposed2 = Sequential()\n",
    "    model_proposed2.add(Conv1D(filters=16, kernel_size=3, padding='causal', activation='relu', input_shape=input_shape))\n",
    "    model_proposed2.add(Bidirectional(LSTM(64, activation='tanh', return_sequences=True)))\n",
    "    model_proposed2.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "    model_proposed2.add(Bidirectional(LSTM(16, activation='tanh', return_sequences=False)))\n",
    "    model_proposed2.add(Dropout(0.1))\n",
    "    model_proposed2.add(Dense(1))\n",
    "    model_proposed2.compile(optimizer='adam', loss='mse')\n",
    "    models['proposed2'] = model_proposed2\n",
    "    \n",
    "    # Proposed Model 3\n",
    "    model_proposed3 = Sequential()\n",
    "    model_proposed3.add(Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model_proposed3.add(Bidirectional(LSTM(64, activation='tanh', return_sequences=True)))\n",
    "    model_proposed3.add(Bidirectional(LSTM(32, activation='tanh', return_sequences=True)))\n",
    "    model_proposed3.add(Bidirectional(LSTM(16, activation='tanh', return_sequences=False)))\n",
    "    model_proposed3.add(Dropout(0.1))\n",
    "    model_proposed3.add(Dense(1))\n",
    "    model_proposed3.compile(optimizer='adam', loss='mse')\n",
    "    models['proposed3'] = model_proposed3\n",
    "\n",
    "    # Proposed Model 4 (WaveNet with LSTM)\n",
    "    def wavenet_layer(inputs, dilation_rate=2, kernel_size=2):\n",
    "        dilated_conv = Conv1D(filters=32, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(inputs)\n",
    "        gated_activation = Activation('tanh')(dilated_conv)\n",
    "        gated_activation = Multiply()([gated_activation, Activation('sigmoid')(dilated_conv)])\n",
    "        return gated_activation\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = wavenet_layer(inputs)\n",
    "    x = wavenet_layer(x, dilation_rate=4)\n",
    "    x = wavenet_layer(x, dilation_rate=8)\n",
    "    x = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(32, activation='tanh', return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(16, activation='tanh', return_sequences=False))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model_proposed4 = Model(inputs=inputs, outputs=outputs)\n",
    "    model_proposed4.compile(optimizer='adam', loss='mse')\n",
    "    models['proposed4'] = model_proposed4\n",
    "\n",
    "    # Proposed Model 5 (WaveNet)\n",
    "    def WaveNet(input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "        # Add more layers as per your WaveNet architecture\n",
    "        outputs = Conv1D(filters=1, kernel_size=1, activation='linear')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    wavenet_input_shape = (X_train.shape[1], X_train.shape[2]) if len(X_train.shape) == 3 else (X_train.shape[1], 1)\n",
    "    wavenet_model = WaveNet(wavenet_input_shape)\n",
    "    models['proposed5'] = wavenet_model\n",
    "\n",
    "    return models\n",
    "\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mslr = tf.keras.metrics.mean_squared_logarithmic_error(y_true, y_pred)\n",
    "\n",
    "    results = {\n",
    "        \"mae\": mae.numpy().mean(),\n",
    "        \"mse\": mse.numpy().mean(),\n",
    "        \"rmse\": rmse.numpy().mean(),\n",
    "        \"mape\": mape.numpy().mean(),\n",
    "        \"mslr\": mslr.numpy().mean(),\n",
    "    }\n",
    "    \n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def process_and_save_results(model_name, column, df, history, y_test_inverse, y_pred_inverse, model, X_train, X_test, feature_transformed):\n",
    "    # Forecasting dates\n",
    "    forecast_dates = pd.date_range(start='2023-01-28', end='2024-12-31', freq='H')\n",
    "    df_forecast = pd.DataFrame(index=forecast_dates)\n",
    "    \n",
    "    # Scale and reshape features\n",
    "    feature_scaled = feature_transformed.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_scaled = scaler.fit_transform(feature_scaled)\n",
    "\n",
    "    forecast_generator = TimeseriesGenerator(feature_scaled, np.zeros(len(feature_scaled)), length=len(X_train), sampling_rate=1, batch_size=1)\n",
    "    predicted_values_forecast = model.predict(forecast_generator)\n",
    "    predicted_values_forecast = scaler.inverse_transform(predicted_values_forecast)\n",
    "\n",
    "    if len(predicted_values_forecast) > len(df_forecast):\n",
    "        predicted_values_forecast = predicted_values_forecast[:len(df_forecast)]\n",
    "    else:\n",
    "        forecast_values = np.full((len(df_forecast), 1), np.nan)\n",
    "        forecast_values[:len(predicted_values_forecast)] = predicted_values_forecast\n",
    "        predicted_values_forecast = forecast_values\n",
    "\n",
    "    df_forecast[column] = predicted_values_forecast\n",
    "    save_to_csv(df_forecast, f'D:/A_NAUSHAD/E/RESULTS/FORE/{model_name}_{column}_Wave_fore.csv')\n",
    "\n",
    "    # Plot forecast\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_forecast.index, df_forecast[column], label='Forecasted')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.title(f'Forecast of Hourly {column} concentration using {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save loss history\n",
    "    save_to_csv(pd.DataFrame(history.history['loss']), f'D:/A_NAUSHAD/E/RESULTS/LOSS/{model_name}_{column}_Wave_loss.csv')\n",
    "    save_to_csv(pd.DataFrame(history.history['val_loss']), f'D:/A_NAUSHAD/E/RESULTS/LOSS/{model_name}_{column}_Wave_val_loss.csv')\n",
    "\n",
    "    # Save training and testing predictions\n",
    "    predictions_train = model.predict(X_train)\n",
    "    save_to_csv(pd.DataFrame(predictions_train), f'D:/A_NAUSHAD/E/RESULTS/PRED/{model_name}_{column}_Wave_train_pred.csv')\n",
    "    \n",
    "    predictions_test = model.predict(X_test)\n",
    "    save_to_csv(pd.DataFrame(predictions_test), f'D:/A_NAUSHAD/E/RESULTS/PRED/{model_name}_{column}_Wave_test_pred.csv')\n",
    "\n",
    "    # Save evaluation results\n",
    "    eval_results = evaluate_preds(y_true=y_test_inverse, y_pred=y_pred_inverse)\n",
    "    eval_df = pd.DataFrame.from_dict(eval_results, orient='index', columns=['value'])\n",
    "    save_to_csv(eval_df, f'D:/A_NAUSHAD/E/RESULTS/EVAL/{model_name}_{column}_Wave_eval.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Define the callbacks\n",
    "lr_monitor = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    factor=0.5,\n",
    "    cooldown=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Preprocess features and target\n",
    "for target_column in feature_columns:\n",
    "    print(f\"\\nProcessing target column: {target_column}\")\n",
    "    \n",
    "    # Prepare target data\n",
    "    target = data[target_column].values\n",
    "    scaler_target = MinMaxScaler()\n",
    "    target_scaled = scaler_target.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "    # Prepare features\n",
    "    features = data[feature_columns].values\n",
    "    scaler_features = MinMaxScaler()\n",
    "    features_scaled = scaler_features.fit_transform(features)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_scaled, test_size=0.15, random_state=1, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=1, shuffle=False)\n",
    "    \n",
    "    # Reshape data for LSTM/GRU input\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Create models\n",
    "    models = create_model((X_train.shape[1], X_train.shape[2]))\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nTraining and evaluating model: {model_name}\")\n",
    "        \n",
    "        # Train model with callbacks\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=200,\n",
    "            batch_size=64,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "            callbacks=[lr_monitor, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_test_inverse = scaler_target.inverse_transform(y_test)\n",
    "        y_pred_inverse = scaler_target.inverse_transform(y_pred)\n",
    "        \n",
    "        # Process and save results\n",
    "        process_and_save_results(model_name, target_column, data, history, y_test_inverse, y_pred_inverse, model, X_train, X_test, features_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee182f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
