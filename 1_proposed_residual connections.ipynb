{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8354ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "WARNING:tensorflow:From C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\rtvip\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "184/184 - 3s - loss: 25.2497 - val_loss: 0.0046 - lr: 0.0100 - 3s/epoch - 16ms/step\n",
      "Epoch 2/100\n",
      "184/184 - 1s - loss: 0.0223 - val_loss: 0.1909 - lr: 0.0100 - 539ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "184/184 - 1s - loss: 0.0395 - val_loss: 9.0951e-08 - lr: 0.0100 - 533ms/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "184/184 - 1s - loss: 2.2491e-06 - val_loss: 1.5107e-08 - lr: 0.0100 - 532ms/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "184/184 - 1s - loss: 2.0564e-06 - val_loss: 1.9627e-08 - lr: 0.0100 - 520ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "184/184 - 1s - loss: 1.9471e-06 - val_loss: 1.1706e-08 - lr: 0.0050 - 550ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "184/184 - 1s - loss: 1.8967e-06 - val_loss: 1.0447e-08 - lr: 0.0050 - 549ms/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "184/184 - 1s - loss: 1.8547e-06 - val_loss: 1.0868e-08 - lr: 0.0025 - 533ms/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "184/184 - 1s - loss: 1.8311e-06 - val_loss: 1.1567e-08 - lr: 0.0025 - 517ms/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "184/184 - 1s - loss: 1.8104e-06 - val_loss: 1.1930e-08 - lr: 0.0012 - 502ms/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "184/184 - 1s - loss: 1.7970e-06 - val_loss: 1.2723e-08 - lr: 0.0012 - 516ms/epoch - 3ms/step\n",
      "Epoch 12/100\n",
      "184/184 - 1s - loss: 1.7853e-06 - val_loss: 1.2688e-08 - lr: 6.2500e-04 - 533ms/epoch - 3ms/step\n",
      "153/153 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
      "Test loss for column Karachi_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 0s 2ms/step\n",
      "X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "Epoch 1/100\n",
      " 438/9024 [>.............................] - ETA: 1:33184/184 - 5s - loss: 42.0342 - val_loss: 0.0102 - lr: 0.0100 - 5s/epoch - 27ms/step\n",
      "Epoch 2/100\n",
      " 545/9024 [>.............................] - ETA: 1:31184/184 - 1s - loss: 0.0215 - val_loss: 1.3313e-06 - lr: 0.0100 - 1s/epoch - 6ms/step\n",
      "Epoch 3/100\n",
      " 649/9024 [=>............................] - ETA: 1:29184/184 - 1s - loss: 1.6370e-06 - val_loss: 1.0222e-06 - lr: 0.0100 - 1s/epoch - 6ms/step\n",
      "Epoch 4/100\n",
      " 750/9024 [=>............................] - ETA: 1:28184/184 - 1s - loss: 1.6298e-06 - val_loss: 1.0621e-06 - lr: 0.0100 - 1s/epoch - 6ms/step\n",
      " 755/9024 [=>............................] - ETA: 1:28Epoch 5/100\n",
      " 851/9024 [=>............................] - ETA: 1:27184/184 - 1s - loss: 1.5634e-06 - val_loss: 1.0672e-06 - lr: 0.0050 - 1s/epoch - 6ms/step\n",
      "Epoch 6/100\n",
      " 947/9024 [==>...........................] - ETA: 1:26184/184 - 1s - loss: 1.5143e-06 - val_loss: 1.0862e-06 - lr: 0.0050 - 1s/epoch - 6ms/step\n",
      "Epoch 7/100\n",
      "1045/9024 [==>...........................] - ETA: 1:26184/184 - 1s - loss: 1.4634e-06 - val_loss: 1.1049e-06 - lr: 0.0025 - 1s/epoch - 6ms/step\n",
      "Epoch 8/100\n",
      "1134/9024 [==>...........................] - ETA: 1:25184/184 - 1s - loss: 1.4707e-06 - val_loss: 1.1031e-06 - lr: 0.0025 - 1s/epoch - 6ms/step\n",
      "153/153 [==============================] - 1s 3ms/step - loss: 0.0000e+00\n",
      "1211/9024 [===>..........................] - ETA: 1:25Test loss for column Islamabad_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "1306/9024 [===>..........................] - ETA: 1:25X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "1340/9024 [===>..........................] - ETA: 1:26Epoch 1/100\n",
      " 406/9024 [>.............................] - ETA: 2:27184/184 - 7s - loss: 182.4300 - val_loss: 0.0192 - lr: 0.0100 - 7s/epoch - 40ms/step\n",
      "Epoch 2/100\n",
      " 482/9024 [>.............................] - ETA: 2:27184/184 - 1s - loss: 0.0058 - val_loss: 4.2673e-04 - lr: 0.0100 - 1s/epoch - 8ms/step\n",
      "Epoch 3/100\n",
      "1924/9024 [=====>........................] - ETA: 1:33184/184 - 2s - loss: 7.4629e-05 - val_loss: 9.0191e-07 - lr: 0.0100 - 2s/epoch - 8ms/step\n",
      "Epoch 4/100\n",
      " 656/9024 [=>............................] - ETA: 2:27184/184 - 2s - loss: 5.8226e-07 - val_loss: 6.1384e-09 - lr: 0.0100 - 2s/epoch - 9ms/step\n",
      "2015/9024 [=====>........................] - ETA: 1:33Epoch 5/100\n",
      " 730/9024 [=>............................] - ETA: 2:28184/184 - 2s - loss: 3.9588e-07 - val_loss: 6.9395e-09 - lr: 0.0100 - 2s/epoch - 8ms/step\n",
      "2087/9024 [=====>........................] - ETA: 1:34Epoch 6/100\n",
      " 800/9024 [=>............................] - ETA: 2:30184/184 - 1s - loss: 3.9425e-07 - val_loss: 9.0236e-09 - lr: 0.0050 - 1s/epoch - 8ms/step\n",
      "Epoch 7/100\n",
      "2224/9024 [======>.......................] - ETA: 1:36184/184 - 1s - loss: 3.9363e-07 - val_loss: 8.3950e-09 - lr: 0.0050 - 1s/epoch - 8ms/step\n",
      "Epoch 8/100\n",
      "2288/9024 [======>.......................] - ETA: 1:37184/184 - 1s - loss: 3.9316e-07 - val_loss: 8.3223e-09 - lr: 0.0025 - 1s/epoch - 8ms/step\n",
      "2291/9024 [======>.......................] - ETA: 1:37Epoch 9/100\n",
      "2356/9024 [======>.......................] - ETA: 1:37184/184 - 2s - loss: 3.9301e-07 - val_loss: 7.7800e-09 - lr: 0.0025 - 2s/epoch - 8ms/step\n",
      "153/153 [==============================] - 1s 5ms/step - loss: 0.0000e+00\n",
      "1049/9024 [==>...........................] - ETA: 2:34Test loss for column Lahore_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 1s 6ms/step\n",
      "2465/9024 [=======>......................] - ETA: 1:39X_train shape: (23493, 1), y_train shape: (23493, \n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "1135/9024 [==>...........................] - ETA: 2:39Epoch 1/100\n",
      "1380/9024 [===>..........................] - ETA: 2:46184/184 - 7s - loss: 72.8674 - val_loss: 0.0187 - lr: 0.0100 - 7s/epoch - 39ms/step\n",
      " 227/9024 [..............................] - ETA: 4:27Epoch 2/100\n",
      "1449/9024 [===>..........................] - ETA: 2:48184/184 - 2s - loss: 0.0436 - val_loss: 6.4512e-04 - lr: 0.0100 - 2s/epoch - 11ms/step\n",
      " 297/9024 [..............................] - ETA: 4:22Epoch 3/100\n",
      "1515/9024 [====>.........................] - ETA: 2:49184/184 - 2s - loss: 0.0197 - val_loss: 0.0038 - lr: 0.0100 - 2s/epoch - 11ms/step\n",
      "Epoch 4/100\n",
      "2931/9024 [========>.....................] - ETA: 1:47184/184 - 2s - loss: 1.5354e-04 - val_loss: 2.1109e-07 - lr: 0.0100 - 2s/epoch - 12ms/step\n",
      "Epoch 5/100\n",
      "2995/9024 [========>.....................] - ETA: 1:48184/184 - 2s - loss: 3.4899e-08 - val_loss: 2.0584e-07 - lr: 0.0100 - 2s/epoch - 12ms/step\n",
      "Epoch 6/100\n",
      " 558/9024 [>.............................] - ETA: 4:28184/184 - 2s - loss: 3.4801e-08 - val_loss: 2.0424e-07 - lr: 0.0100 - 2s/epoch - 13ms/step\n",
      "Epoch 7/100\n",
      " 620/9024 [=>............................] - ETA: 4:30184/184 - 2s - loss: 3.4813e-08 - val_loss: 2.0529e-07 - lr: 0.0050 - 2s/epoch - 13ms/step\n",
      "3123/9024 [=========>....................] - ETA: 1:50Epoch 8/100\n",
      "3188/9024 [=========>....................] - ETA: 1:51184/184 - 2s - loss: 3.4800e-08 - val_loss: 2.0477e-07 - lr: 0.0050 - 2s/epoch - 13ms/step\n",
      "1844/9024 [=====>........................] - ETA: 2:58Epoch 9/100\n",
      "3253/9024 [=========>....................] - ETA: 1:52184/184 - 2s - loss: 3.4800e-08 - val_loss: 2.0480e-07 - lr: 0.0025 - 2s/epoch - 13ms/step\n",
      "Epoch 10/100\n",
      "3317/9024 [==========>...................] - ETA: 1:53184/184 - 3s - loss: 3.4810e-08 - val_loss: 2.0519e-07 - lr: 0.0025 - 3s/epoch - 14ms/step\n",
      "Epoch 11/100\n",
      "3375/9024 [==========>...................] - ETA: 1:54184/184 - 2s - loss: 3.4790e-08 - val_loss: 2.0509e-07 - lr: 0.0012 - 2s/epoch - 13ms/step\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0000e+00\n",
      "Test loss for column Peshawar_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 2s 10ms/step\n",
      "3460/9024 [==========>...................] - ETA: 1:56X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      " 969/9024 [==>...........................] - ETA: 4:47Epoch 1/100\n",
      "1143/9024 [==>...........................] - ETA: 4:59184/184 - 9s - loss: 230.6300 - val_loss: 0.0487 - lr: 0.0100 - 9s/epoch - 48ms/step\n",
      "Epoch 2/100\n",
      " 230/9024 [..............................] - ETA: 7:18184/184 - 3s - loss: 0.0553 - val_loss: 3.5950e-05 - lr: 0.0100 - 3s/epoch - 17ms/step\n",
      "2370/9024 [======>.......................] - ETA: 3:16Epoch 3/100\n",
      "1272/9024 [===>..........................] - ETA: 5:05184/184 - 4s - loss: 8.9459e-06 - val_loss: 2.0175e-07 - lr: 0.0100 - 4s/epoch - 20ms/step\n",
      "Epoch 4/100\n",
      "1339/9024 [===>..........................] - ETA: 5:11184/184 - 4s - loss: 8.5348e-07 - val_loss: 1.2565e-07 - lr: 0.0100 - 4s/epoch - 22ms/step\n",
      "2504/9024 [=======>......................] - ETA: 3:21Epoch 5/100\n",
      "1410/9024 [===>..........................] - ETA: 5:15184/184 - 4s - loss: 7.3185e-07 - val_loss: 1.3649e-07 - lr: 0.0050 - 4s/epoch - 23ms/step\n",
      " 438/9024 [>.............................] - ETA: 7:38Epoch 6/100\n",
      "2640/9024 [=======>......................] - ETA: 3:29184/184 - 5s - loss: 7.2788e-07 - val_loss: 1.3095e-07 - lr: 0.0050 - 5s/epoch - 26ms/step\n",
      " 514/9024 [>.............................] - ETA: 7:46Epoch 7/100\n",
      " 592/9024 [>.............................] - ETA: 7:56184/184 - 5s - loss: 6.6988e-07 - val_loss: 1.6339e-07 - lr: 0.0025 - 5s/epoch - 28ms/step\n",
      "4052/9024 [============>.................] - ETA: 2:11Epoch 8/100\n",
      " 664/9024 [=>............................] - ETA: 8:04184/184 - 5s - loss: 6.4974e-07 - val_loss: 1.1112e-07 - lr: 0.0025 - 5s/epoch - 27ms/step\n",
      "1629/9024 [====>.........................] - ETA: 5:33Epoch 9/100\n",
      "2859/9024 [========>.....................] - ETA: 3:41184/184 - 6s - loss: 6.3566e-07 - val_loss: 1.3858e-07 - lr: 0.0012 - 6s/epoch - 31ms/step\n",
      "Epoch 10/100\n",
      "1781/9024 [====>.........................] - ETA: 5:53184/184 - 7s - loss: 6.3740e-07 - val_loss: 1.4786e-07 - lr: 0.0012 - 7s/epoch - 41ms/step\n",
      "4272/9024 [=============>................] - ETA: 2:19Epoch 11/100\n",
      "3026/9024 [=========>....................] - ETA: 3:56184/184 - 9s - loss: 6.1278e-07 - val_loss: 1.2470e-07 - lr: 6.2500e-04 - 9s/epoch - 49ms/step\n",
      "4361/9024 [=============>................] - ETA: 2:23Epoch 12/100\n",
      " 993/9024 [==>...........................] - ETA: 9:32184/184 - 10s - loss: 6.1355e-07 - val_loss: 1.2833e-07 - lr: 6.2500e-04 - 10s/epoch - 54ms/step\n",
      "3114/9024 [=========>....................] - ETA: 4:05Epoch 13/100\n",
      "3205/9024 [=========>....................] - ETA: 4:12184/184 - 10s - loss: 6.0449e-07 - val_loss: 1.2187e-07 - lr: 3.1250e-04 - 10s/epoch - 55ms/step\n",
      "153/153 [==============================] - 3s 14ms/step - loss: 0.0000e+00\n",
      "Test loss for column Dhaka_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 4s 26ms/step\n",
      "4610/9024 [==============>...............] - ETA: 2:35X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "3281/9024 [=========>....................] - ETA: 4:19Epoch 1/100\n",
      "3435/9024 [==========>...................] - ETA: 4:27184/184 - 16s - loss: 45.6934 - val_loss: 5.7666e-04 - lr: 0.0100 - 16s/epoch - 88ms/step\n",
      "2279/9024 [======>.......................] - ETA: 6:57Epoch 2/100\n",
      "4862/9024 [===============>..............] - ETA: 2:42184/184 - 10s - loss: 9.0406e-04 - val_loss: 7.0494e-07 - lr: 0.0100 - 10s/epoch - 55ms/step\n",
      "1405/9024 [===>..........................] - ETA: 10:29Epoch 3/100\n",
      "3614/9024 [===========>..................] - ETA: 4:37184/184 - 11s - loss: 8.7226e-07 - val_loss: 9.1641e-07 - lr: 0.0100 - 11s/epoch - 58ms/step\n",
      "4951/9024 [===============>..............] - ETA: 2:45Epoch 4/100\n",
      "3701/9024 [===========>..................] - ETA: 4:42184/184 - 11s - loss: 1.3784e-06 - val_loss: 8.2281e-06 - lr: 0.0100 - 11s/epoch - 61ms/step\n",
      "Epoch 5/100\n",
      "5124/9024 [================>.............] - ETA: 2:49184/184 - 11s - loss: 1.0506e-06 - val_loss: 6.4018e-07 - lr: 0.0050 - 11s/epoch - 61ms/step\n",
      "2629/9024 [=======>......................] - ETA: 7:28Epoch 6/100\n",
      "1749/9024 [====>.........................] - ETA: 11:09184/184 - 12s - loss: 4.2427e-07 - val_loss: 3.3244e-07 - lr: 0.0050 - 12s/epoch - 63ms/step\n",
      "Epoch 7/100\n",
      "1830/9024 [=====>........................] - ETA: 11:19184/184 - 12s - loss: 2.5780e-07 - val_loss: 3.3327e-07 - lr: 0.0025 - 12s/epoch - 65ms/step\n",
      "Epoch 8/100\n",
      "5371/9024 [================>.............] - ETA: 2:56184/184 - 13s - loss: 2.5101e-07 - val_loss: 3.0917e-07 - lr: 0.0025 - 13s/epoch - 70ms/step\n",
      "Epoch 9/100\n",
      "2955/9024 [========>.....................] - ETA: 8:00184/184 - 13s - loss: 1.9683e-07 - val_loss: 2.5357e-07 - lr: 0.0012 - 13s/epoch - 71ms/step\n",
      "Epoch 10/100\n",
      "4190/9024 [============>.................] - ETA: 5:12184/184 - 13s - loss: 1.8317e-07 - val_loss: 3.3428e-07 - lr: 0.0012 - 13s/epoch - 73ms/step\n",
      "Epoch 11/100\n",
      "3114/9024 [=========>....................] - ETA: 8:15184/184 - 14s - loss: 1.8027e-07 - val_loss: 2.2699e-07 - lr: 6.2500e-04 - 14s/epoch - 75ms/step\n",
      "Epoch 12/100\n",
      "5694/9024 [=================>............] - ETA: 3:03184/184 - 15s - loss: 1.6428e-07 - val_loss: 3.3950e-07 - lr: 6.2500e-04 - 15s/epoch - 80ms/step\n",
      "2235/9024 [======>.......................] - ETA: 12:12Epoch 13/100\n",
      "5775/9024 [==================>...........] - ETA: 3:05184/184 - 15s - loss: 1.6044e-07 - val_loss: 2.2582e-07 - lr: 3.1250e-04 - 15s/epoch - 82ms/step\n",
      "Epoch 14/100\n",
      "3359/9024 [==========>...................] - ETA: 8:36184/184 - 15s - loss: 1.5785e-07 - val_loss: 2.2441e-07 - lr: 3.1250e-04 - 15s/epoch - 84ms/step\n",
      "4513/9024 [==============>...............] - ETA: 5:29Epoch 15/100\n",
      "5936/9024 [==================>...........] - ETA: 3:07184/184 - 16s - loss: 1.5057e-07 - val_loss: 2.2703e-07 - lr: 1.5625e-04 - 16s/epoch - 87ms/step\n",
      "3439/9024 [==========>...................] - ETA: 8:43Epoch 16/100\n",
      "6015/9024 [==================>...........] - ETA: 3:08184/184 - 17s - loss: 1.4975e-07 - val_loss: 2.2659e-07 - lr: 1.5625e-04 - 17s/epoch - 90ms/step\n",
      "Epoch 17/100\n",
      "4753/9024 [==============>...............] - ETA: 5:41184/184 - 17s - loss: 1.4845e-07 - val_loss: 2.2583e-07 - lr: 7.8125e-05 - 17s/epoch - 94ms/step\n",
      "Epoch 18/100\n",
      "3679/9024 [===========>..................] - ETA: 9:01184/184 - 17s - loss: 1.4733e-07 - val_loss: 2.2646e-07 - lr: 7.8125e-05 - 17s/epoch - 90ms/step\n",
      "2719/9024 [========>.....................] - ETA: 13:03Epoch 19/100\n",
      "4910/9024 [===============>..............] - ETA: 5:46184/184 - 17s - loss: 1.4580e-07 - val_loss: 2.2653e-07 - lr: 3.9062e-05 - 17s/epoch - 94ms/step\n",
      "153/153 [==============================] - 2s 11ms/step - loss: 0.0000e+00\n",
      "6264/9024 [===================>..........] - ETA: 3:10Test loss for column Beijing_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 6s 36ms/step\n",
      "4951/9024 [===============>..............] - ETA: 5:47X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "2843/9024 [========>.....................] - ETA: 13:14Epoch 1/100\n",
      "3909/9024 [===========>..................] - ETA: 9:13184/184 - 23s - loss: 8.3949 - val_loss: 6.7129e-06 - lr: 0.0100 - 23s/epoch - 125ms/step\n",
      "Epoch 2/100\n",
      "5137/9024 [================>.............] - ETA: 5:50184/184 - 17s - loss: 0.0181 - val_loss: 3.6829e-06 - lr: 0.0100 - 17s/epoch - 93ms/step\n",
      "Epoch 3/100\n",
      "3103/9024 [=========>....................] - ETA: 13:27184/184 - 18s - loss: 1.2334e-06 - val_loss: 1.5961e-06 - lr: 0.0100 - 18s/epoch - 97ms/step\n",
      "4063/9024 [============>.................] - ETA: 9:19Epoch 4/100\n",
      "3179/9024 [=========>....................] - ETA: 13:31184/184 - 18s - loss: 7.3867e-07 - val_loss: 1.2197e-06 - lr: 0.0050 - 18s/epoch - 100ms/step\n",
      "Epoch 5/100\n",
      "4216/9024 [=============>................] - ETA: 9:25184/184 - 19s - loss: 7.8694e-07 - val_loss: 1.3351e-06 - lr: 0.0050 - 19s/epoch - 104ms/step\n",
      "Epoch 6/100\n",
      "6782/9024 [=====================>........] - ETA: 3:03184/184 - 19s - loss: 6.9560e-07 - val_loss: 1.2957e-06 - lr: 0.0025 - 19s/epoch - 104ms/step\n",
      "4291/9024 [=============>................] - ETA: 9:28Epoch 7/100\n",
      "5522/9024 [=================>............] - ETA: 5:54184/184 - 20s - loss: 6.7693e-07 - val_loss: 1.2638e-06 - lr: 0.0025 - 20s/epoch - 109ms/step\n",
      "Epoch 8/100\n",
      "3482/9024 [==========>...................] - ETA: 13:48184/184 - 20s - loss: 6.6250e-07 - val_loss: 1.2480e-06 - lr: 0.0012 - 20s/epoch - 110ms/step\n",
      "6935/9024 [======================>.......] - ETA: 2:58Epoch 9/100\n",
      "7010/9024 [======================>.......] - ETA: 2:56184/184 - 21s - loss: 6.5400e-07 - val_loss: 1.2293e-06 - lr: 0.0012 - 21s/epoch - 114ms/step\n",
      "153/153 [==============================] - 3s 15ms/step - loss: 0.0000e+00\n",
      "Test loss for column Shanghai_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 8s 48ms/step\n",
      "3599/9024 [==========>...................] - ETA: 13:53_train shape: (23493, 1), y_train shape: (23493, 1\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "7054/9024 [======================>.......] - ETA: 2:55Epoch 1/100\n",
      "5819/9024 [==================>...........] - ETA: 5:52184/184 - 27s - loss: 14.3641 - val_loss: 2.4071e-04 - lr: 0.0100 - 27s/epoch - 147ms/step\n",
      "Epoch 2/100\n",
      "4743/9024 [==============>...............] - ETA: 9:35184/184 - 21s - loss: 5.9391e-05 - val_loss: 1.4368e-05 - lr: 0.0100 - 21s/epoch - 116ms/step\n",
      "Epoch 3/100\n",
      "3864/9024 [===========>..................] - ETA: 13:55184/184 - 23s - loss: 2.3098e-05 - val_loss: 1.2543e-05 - lr: 0.0100 - 23s/epoch - 127ms/step\n",
      "Epoch 4/100\n",
      "4905/9024 [===============>..............] - ETA: 9:35184/184 - 24s - loss: 1.9360e-04 - val_loss: 1.2730e-05 - lr: 0.0100 - 24s/epoch - 131ms/step\n",
      "Epoch 5/100\n",
      "6139/9024 [===================>..........] - ETA: 5:44184/184 - 24s - loss: 1.7746e-05 - val_loss: 9.3804e-06 - lr: 0.0050 - 24s/epoch - 131ms/step\n",
      "Epoch 6/100\n",
      "5063/9024 [===============>..............] - ETA: 9:33184/184 - 25s - loss: 1.5625e-05 - val_loss: 8.1399e-06 - lr: 0.0050 - 25s/epoch - 135ms/step\n",
      "7554/9024 [========================>.....] - ETA: 2:30Epoch 7/100\n",
      "7634/9024 [========================>.....] - ETA: 2:25184/184 - 26s - loss: 1.4893e-05 - val_loss: 7.5118e-06 - lr: 0.0025 - 26s/epoch - 142ms/step\n",
      "Epoch 8/100\n",
      "7712/9024 [========================>.....] - ETA: 2:20184/184 - 26s - loss: 1.4019e-05 - val_loss: 6.8699e-06 - lr: 0.0025 - 26s/epoch - 141ms/step\n",
      "Epoch 9/100\n",
      "6451/9024 [====================>.........] - ETA: 5:33184/184 - 26s - loss: 1.3469e-05 - val_loss: 6.5880e-06 - lr: 0.0012 - 26s/epoch - 144ms/step\n",
      "Epoch 10/100\n",
      "7868/9024 [=========================>....] - ETA: 2:09184/184 - 27s - loss: 1.2889e-05 - val_loss: 6.1747e-06 - lr: 0.0012 - 27s/epoch - 149ms/step\n",
      "Epoch 11/100\n",
      "4491/9024 [=============>................] - ETA: 13:58184/184 - 26s - loss: 1.2345e-05 - val_loss: 5.7952e-06 - lr: 6.2500e-04 - 26s/epoch - 144ms/step\n",
      "Epoch 12/100\n",
      "6683/9024 [=====================>........] - ETA: 5:21184/184 - 28s - loss: 1.1616e-05 - val_loss: 5.4441e-06 - lr: 6.2500e-04 - 28s/epoch - 152ms/step\n",
      "Epoch 13/100\n",
      "6762/9024 [=====================>........] - ETA: 5:16184/184 - 29s - loss: 1.1151e-05 - val_loss: 5.3129e-06 - lr: 3.1250e-04 - 29s/epoch - 159ms/step\n",
      "Epoch 14/100\n",
      "6838/9024 [=====================>........] - ETA: 5:12184/184 - 29s - loss: 1.0912e-05 - val_loss: 5.1580e-06 - lr: 3.1250e-04 - 29s/epoch - 160ms/step\n",
      "Epoch 15/100\n",
      "4802/9024 [==============>...............] - ETA: 13:53184/184 - 30s - loss: 1.0557e-05 - val_loss: 5.1246e-06 - lr: 1.5625e-04 - 30s/epoch - 164ms/step\n",
      "6914/9024 [=====================>........] - ETA: 5:07Epoch 16/100\n",
      "8330/9024 [==========================>...] - ETA: 1:27184/184 - 31s - loss: 1.0349e-05 - val_loss: 5.0171e-06 - lr: 1.5625e-04 - 31s/epoch - 168ms/step\n",
      "6992/9024 [======================>.......] - ETA: 5:01Epoch 17/100\n",
      "5919/9024 [==================>...........] - ETA: 9:08184/184 - 31s - loss: 1.0183e-05 - val_loss: 4.9937e-06 - lr: 7.8125e-05 - 31s/epoch - 168ms/step\n",
      "Epoch 18/100\n",
      "5039/9024 [===============>..............] - ETA: 13:44184/184 - 32s - loss: 1.0092e-05 - val_loss: 4.9425e-06 - lr: 7.8125e-05 - 32s/epoch - 175ms/step\n",
      "Epoch 19/100\n",
      "8563/9024 [===========================>..] - ETA: 1:01184/184 - 33s - loss: 1.0017e-05 - val_loss: 4.9363e-06 - lr: 3.9062e-05 - 33s/epoch - 177ms/step\n",
      "Epoch 20/100\n",
      "8640/9024 [===========================>..] - ETA: 52s184/184 - 33s - loss: 9.9718e-06 - val_loss: 4.9050e-06 - lr: 3.9062e-05 - 33s/epoch - 178ms/step\n",
      "Epoch 21/100\n",
      "5268/9024 [================>.............] - ETA: 13:33184/184 - 34s - loss: 9.7598e-06 - val_loss: 4.8602e-06 - lr: 1.9531e-05 - 34s/epoch - 182ms/step\n",
      "7377/9024 [=======================>......] - ETA: 4:27Epoch 22/100\n",
      "8794/9024 [============================>.] - ETA: 32s2184/184 - 34s - loss: 9.3875e-06 - val_loss: 4.8204e-06 - lr: 1.9531e-05 - 34s/epoch - 186ms/step\n",
      "Epoch 23/100\n",
      "6380/9024 [====================>.........] - ETA: 8:36184/184 - 35s - loss: 9.1307e-06 - val_loss: 4.8008e-06 - lr: 9.7656e-06 - 35s/epoch - 191ms/step\n",
      "Epoch 24/100\n",
      "8952/9024 [============================>.] - ETA: 10s1184/184 - 34s - loss: 8.9684e-06 - val_loss: 4.7831e-06 - lr: 9.7656e-06 - 34s/epoch - 183ms/step\n",
      "Epoch 25/100\n",
      "9024/9024 [==============================] - 1345s 149ms/step\n",
      "5583/9024 [=================>............] - ETA: 13:05184/184 - 30s - loss: 8.8491e-06 - val_loss: 4.7753e-06 - lr: 4.8828e-06 - 30s/epoch - 165ms/step\n",
      "Epoch 26/100\n",
      "6639/9024 [=====================>........] - ETA: 8:01184/184 - 30s - loss: 8.7701e-06 - val_loss: 4.7652e-06 - lr: 4.8828e-06 - 30s/epoch - 164ms/step\n",
      "Epoch 27/100\n",
      "6740/9024 [=====================>........] - ETA: 7:44184/184 - 30s - loss: 8.7084e-06 - val_loss: 4.7609e-06 - lr: 2.4414e-06 - 30s/epoch - 162ms/step\n",
      "Epoch 28/100\n",
      "  40/9024 [..............................] - ETA: 1:00:49184/184 - 33s - loss: 8.6661e-06 - val_loss: 4.7555e-06 - lr: 2.4414e-06 - 33s/epoch - 180ms/step\n",
      "Epoch 29/100\n",
      "6909/9024 [=====================>........] - ETA: 7:18184/184 - 30s - loss: 8.6329e-06 - val_loss: 4.7535e-06 - lr: 1.2207e-06 - 30s/epoch - 165ms/step\n",
      "8061/9024 [=========================>....] - ETA: 2:53Epoch 30/100\n",
      "6989/9024 [======================>.......] - ETA: 7:05184/184 - 28s - loss: 8.6098e-06 - val_loss: 4.7502e-06 - lr: 1.2207e-06 - 28s/epoch - 154ms/step\n",
      "Epoch 31/100\n",
      "7071/9024 [======================>.......] - ETA: 6:50184/184 - 26s - loss: 8.5907e-06 - val_loss: 4.7491e-06 - lr: 6.1035e-07 - 26s/epoch - 141ms/step\n",
      "Epoch 32/100\n",
      " 362/9024 [>.............................] - ETA: 49:52184/184 - 24s - loss: 8.5783e-06 - val_loss: 4.7476e-06 - lr: 6.1035e-07 - 24s/epoch - 131ms/step\n",
      "Epoch 33/100\n",
      "8388/9024 [==========================>...] - ETA: 1:57184/184 - 22s - loss: 8.5684e-06 - val_loss: 4.7475e-06 - lr: 3.0518e-07 - 22s/epoch - 118ms/step\n",
      "Epoch 34/100\n",
      "6366/9024 [====================>.........] - ETA: 10:33184/184 - 20s - loss: 8.5616e-06 - val_loss: 4.7467e-06 - lr: 3.0518e-07 - 20s/epoch - 107ms/step\n",
      "Epoch 35/100\n",
      "7405/9024 [=======================>......] - ETA: 5:43184/184 - 17s - loss: 8.5562e-06 - val_loss: 4.7465e-06 - lr: 1.5259e-07 - 17s/epoch - 93ms/step\n",
      " 614/9024 [=>............................] - ETA: 41:54Epoch 36/100\n",
      "7488/9024 [=======================>......] - ETA: 5:25184/184 - 17s - loss: 8.5523e-06 - val_loss: 4.7461e-06 - lr: 1.5259e-07 - 17s/epoch - 94ms/step\n",
      "Epoch 37/100\n",
      " 781/9024 [=>............................] - ETA: 38:18184/184 - 17s - loss: 8.5492e-06 - val_loss: 4.7460e-06 - lr: 7.6294e-08 - 17s/epoch - 93ms/step\n",
      "Epoch 38/100\n",
      "7655/9024 [========================>.....] - ETA: 4:49184/184 - 17s - loss: 8.5470e-06 - val_loss: 4.7458e-06 - lr: 7.6294e-08 - 17s/epoch - 95ms/step\n",
      "Epoch 39/100\n",
      "8892/9024 [============================>.] - ETA: 24s1184/184 - 20s - loss: 8.5453e-06 - val_loss: 4.7457e-06 - lr: 3.8147e-08 - 20s/epoch - 111ms/step\n",
      "Epoch 40/100\n",
      "1031/9024 [==>...........................] - ETA: 36:21184/184 - 26s - loss: 8.5443e-06 - val_loss: 4.7456e-06 - lr: 3.8147e-08 - 26s/epoch - 140ms/step\n",
      "Epoch 41/100\n",
      "9024/9024 [==============================] - 1709s 189ms/step\n",
      "1125/9024 [==>...........................] - ETA: 36:21184/184 - 29s - loss: 8.5435e-06 - val_loss: 4.7456e-06 - lr: 1.9073e-08 - 29s/epoch - 159ms/step\n",
      "Epoch 42/100\n",
      "8020/9024 [=========================>....] - ETA: 3:35184/184 - 25s - loss: 8.5429e-06 - val_loss: 4.7455e-06 - lr: 1.9073e-08 - 25s/epoch - 137ms/step\n",
      "Epoch 43/100\n",
      "7164/9024 [======================>.......] - ETA: 7:24184/184 - 24s - loss: 8.5425e-06 - val_loss: 4.7455e-06 - lr: 9.5367e-09 - 24s/epoch - 130ms/step\n",
      "Epoch 44/100\n",
      "1417/9024 [===>..........................] - ETA: 34:15184/184 - 23s - loss: 8.5424e-06 - val_loss: 4.7455e-06 - lr: 9.5367e-09 - 23s/epoch - 125ms/step\n",
      "Epoch 45/100\n",
      " 184/9024 [..............................] - ETA: 38:07184/184 - 21s - loss: 8.5423e-06 - val_loss: 4.7455e-06 - lr: 4.7684e-09 - 21s/epoch - 114ms/step\n",
      "Epoch 46/100\n",
      "7424/9024 [=======================>......] - ETA: 6:22184/184 - 19s - loss: 8.5422e-06 - val_loss: 4.7455e-06 - lr: 4.7684e-09 - 19s/epoch - 102ms/step\n",
      "1592/9024 [====>.........................] - ETA: 32:53Epoch 47/100\n",
      " 361/9024 [>.............................] - ETA: 32:59184/184 - 16s - loss: 8.5422e-06 - val_loss: 4.7455e-06 - lr: 2.3842e-09 - 16s/epoch - 88ms/step\n",
      "8471/9024 [===========================>..] - ETA: 1:59Epoch 48/100\n",
      " 449/9024 [>.............................] - ETA: 30:40184/184 - 14s - loss: 8.5422e-06 - val_loss: 4.7455e-06 - lr: 2.3842e-09 - 14s/epoch - 75ms/step\n",
      "Epoch 49/100\n",
      "7690/9024 [========================>.....] - ETA: 5:15184/184 - 13s - loss: 8.5421e-06 - val_loss: 4.7455e-06 - lr: 1.1921e-09 - 13s/epoch - 70ms/step\n",
      "Epoch 50/100\n",
      "1946/9024 [=====>........................] - ETA: 28:59184/184 - 13s - loss: 8.5421e-06 - val_loss: 4.7455e-06 - lr: 1.1921e-09 - 13s/epoch - 69ms/step\n",
      " 627/9024 [=>............................] - ETA: 27:12Epoch 51/100\n",
      "8824/9024 [============================>.] - ETA: 42s1184/184 - 13s - loss: 8.5421e-06 - val_loss: 4.7455e-06 - lr: 5.9605e-10 - 13s/epoch - 73ms/step\n",
      "Epoch 52/100\n",
      "8911/9024 [============================>.] - ETA: 24s3184/184 - 17s - loss: 8.5421e-06 - val_loss: 4.7455e-06 - lr: 5.9605e-10 - 17s/epoch - 93ms/step\n",
      "Epoch 53/100\n",
      "8997/9024 [============================>.] - ETA: 5s:184/184 - 22s - loss: 8.5421e-06 - val_loss: 4.7455e-06 - lr: 2.9802e-10 - 22s/epoch - 122ms/step\n",
      "Epoch 54/100\n",
      "9024/9024 [==============================] - 1926s 213ms/step\n",
      "8145/9024 [==========================>...] - ETA: 3:25184/184 - 22s - loss: 8.5421e-06 - val_loss: 4.7455e-06 - lr: 2.9802e-10 - 22s/epoch - 120ms/step\n",
      "Epoch 55/100\n",
      "2412/9024 [=======>......................] - ETA: 26:11184/184 - 20s - loss: 8.5421e-06 - val_loss: 4.7455e-06 - lr: 1.4901e-10 - 20s/epoch - 109ms/step\n",
      "153/153 [==============================] - 1s 7ms/step - loss: 0.0000e+00\n",
      "1105/9024 [==>...........................] - ETA: 26:06Test loss for column Chennai_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 6s 35ms/step\n",
      "2463/9024 [=======>......................] - ETA: 25:48X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "2469/9024 [=======>......................] - ETA: 25:45Epoch 1/100\n",
      "  77/9024 [..............................] - ETA: 23:48184/184 - 19s - loss: 178.5787 - val_loss: 0.0110 - lr: 0.0100 - 19s/epoch - 102ms/step\n",
      "Epoch 2/100\n",
      "2680/9024 [=======>......................] - ETA: 24:10184/184 - 12s - loss: 0.0032 - val_loss: 5.5792e-05 - lr: 0.0100 - 12s/epoch - 66ms/step\n",
      "Epoch 3/100\n",
      "2768/9024 [========>.....................] - ETA: 23:26184/184 - 10s - loss: 1.1088e-05 - val_loss: 8.5820e-07 - lr: 0.0100 - 10s/epoch - 53ms/step\n",
      "Epoch 4/100\n",
      " 347/9024 [>.............................] - ETA: 17:39184/184 - 8s - loss: 1.5355e-06 - val_loss: 8.4202e-07 - lr: 0.0100 - 8s/epoch - 45ms/step\n",
      "8698/9024 [===========================>..] - ETA: 1:14Epoch 5/100\n",
      " 441/9024 [>.............................] - ETA: 16:22184/184 - 8s - loss: 1.5290e-06 - val_loss: 7.7861e-07 - lr: 0.0050 - 8s/epoch - 44ms/step\n",
      "Epoch 6/100\n",
      "1730/9024 [====>.........................] - ETA: 20:32184/184 - 10s - loss: 1.5201e-06 - val_loss: 7.9910e-07 - lr: 0.0050 - 10s/epoch - 53ms/step\n",
      "Epoch 7/100\n",
      " 616/9024 [=>............................] - ETA: 16:41184/184 - 13s - loss: 1.5157e-06 - val_loss: 7.8221e-07 - lr: 0.0025 - 13s/epoch - 71ms/step\n",
      "Epoch 8/100\n",
      "9024/9024 [==============================] - 2023s 224ms/step\n",
      "3230/9024 [=========>....................] - ETA: 20:13184/184 - 14s - loss: 1.5131e-06 - val_loss: 8.0096e-07 - lr: 0.0025 - 14s/epoch - 79ms/step\n",
      "1915/9024 [=====>........................] - ETA: 19:47Epoch 9/100\n",
      " 820/9024 [=>............................] - ETA: 16:53184/184 - 13s - loss: 1.5057e-06 - val_loss: 7.9466e-07 - lr: 0.0012 - 13s/epoch - 73ms/step\n",
      "Epoch 10/100\n",
      " 926/9024 [==>...........................] - ETA: 16:41184/184 - 13s - loss: 1.5068e-06 - val_loss: 8.7080e-07 - lr: 0.0012 - 13s/epoch - 72ms/step\n",
      "153/153 [==============================] - 2s 12ms/step - loss: 0.0000e+00\n",
      "3466/9024 [==========>...................] - ETA: 18:51.\n",
      "153/153 [==============================] - 3s 20ms/step\n",
      "3513/9024 [==========>...................] - ETA: 18:32X_train shape: (23493, 1), y_train shape: (23493, 1)\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "3521/9024 [==========>...................] - ETA: 18:29Epoch 1/100\n",
      "2361/9024 [======>.......................] - ETA: 17:07184/184 - 11s - loss: 101.5877 - val_loss: 0.0088 - lr: 0.0100 - 11s/epoch - 57ms/step\n",
      " 118/9024 [..............................] - ETA: 9:41Epoch 2/100\n",
      "1250/9024 [===>..........................] - ETA: 14:04184/184 - 4s - loss: 0.0149 - val_loss: 2.0909e-06 - lr: 0.0100 - 4s/epoch - 21ms/step\n",
      "2451/9024 [=======>......................] - ETA: 16:26Epoch 3/100\n",
      "2536/9024 [=======>......................] - ETA: 15:51184/184 - 4s - loss: 6.1648e-05 - val_loss: 2.4907e-06 - lr: 0.0100 - 4s/epoch - 22ms/step\n",
      "Epoch 4/100\n",
      "3953/9024 [============>.................] - ETA: 15:42184/184 - 7s - loss: 6.0379e-05 - val_loss: 2.5357e-06 - lr: 0.0100 - 7s/epoch - 37ms/step\n",
      "Epoch 5/100\n",
      "1533/9024 [====>.........................] - ETA: 12:29184/184 - 7s - loss: 5.9649e-05 - val_loss: 2.8378e-06 - lr: 0.0050 - 7s/epoch - 37ms/step\n",
      " 484/9024 [>.............................] - ETA: 8:33Epoch 6/100\n",
      "4144/9024 [============>.................] - ETA: 14:41184/184 - 7s - loss: 5.9198e-05 - val_loss: 2.6879e-06 - lr: 0.0050 - 7s/epoch - 36ms/step\n",
      "Epoch 7/100\n",
      "1725/9024 [====>.........................] - ETA: 11:45184/184 - 7s - loss: 5.8736e-05 - val_loss: 2.5786e-06 - lr: 0.0025 - 7s/epoch - 37ms/step\n",
      "153/153 [==============================] - 2s 9ms/step - loss: 0.0000e+00\n",
      "Test loss for column Kolkata_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 3s 18ms/step\n",
      "1790/9024 [====>.........................] - ETA: 11:36X_train shape: (23493, 1), y_train shape: (23493, 1\n",
      "X_val shape: (4146, 1), y_val shape: (4146, 1)\n",
      "X_test shape: (4878, 1), y_test shape: (4878, 1)\n",
      "4312/9024 [=============>................] - ETA: 13:52Epoch 1/100\n",
      "4460/9024 [=============>................] - ETA: 13:12184/184 - 13s - loss: 33.3110 - val_loss: 0.0022 - lr: 0.0100 - 13s/epoch - 72ms/step\n",
      "Epoch 2/100\n",
      " 985/9024 [==>...........................] - ETA: 9:31184/184 - 8s - loss: 2.4095e-04 - val_loss: 6.3317e-06 - lr: 0.0100 - 8s/epoch - 43ms/step\n",
      "3230/9024 [=========>....................] - ETA: 12:44Epoch 3/100\n",
      "1077/9024 [==>...........................] - ETA: 9:43184/184 - 9s - loss: 2.7147e-05 - val_loss: 6.1674e-06 - lr: 0.0100 - 9s/epoch - 49ms/step\n",
      "4643/9024 [==============>...............] - ETA: 12:27Epoch 4/100\n",
      "4729/9024 [==============>...............] - ETA: 12:08184/184 - 10s - loss: 2.6837e-05 - val_loss: 6.2353e-06 - lr: 0.0100 - 10s/epoch - 54ms/step\n",
      "2213/9024 [======>.......................] - ETA: 10:56Epoch 5/100\n",
      "2300/9024 [======>.......................] - ETA: 10:54184/184 - 11s - loss: 2.3547e-04 - val_loss: 6.8453e-07 - lr: 0.0050 - 11s/epoch - 57ms/step\n",
      "Epoch 6/100\n",
      "1339/9024 [===>..........................] - ETA: 10:33184/184 - 11s - loss: 5.6000e-06 - val_loss: 1.6376e-06 - lr: 0.0050 - 11s/epoch - 58ms/step\n",
      "4905/9024 [===============>..............] - ETA: 11:31Epoch 7/100\n",
      "4990/9024 [===============>..............] - ETA: 11:14184/184 - 11s - loss: 4.6439e-06 - val_loss: 1.1327e-06 - lr: 0.0025 - 11s/epoch - 59ms/step\n",
      "1425/9024 [===>..........................] - ETA: 10:46Epoch 8/100\n",
      "5074/9024 [===============>..............] - ETA: 10:57184/184 - 11s - loss: 4.3454e-06 - val_loss: 1.0243e-06 - lr: 0.0025 - 11s/epoch - 60ms/step\n",
      "1510/9024 [====>.........................] - ETA: 10:58Epoch 9/100\n",
      "1596/9024 [====>.........................] - ETA: 11:11184/184 - 12s - loss: 3.8813e-06 - val_loss: 1.1267e-06 - lr: 0.0012 - 12s/epoch - 66ms/step\n",
      "Epoch 10/100\n",
      "5245/9024 [================>.............] - ETA: 10:27184/184 - 14s - loss: 3.6887e-06 - val_loss: 1.1068e-06 - lr: 0.0012 - 14s/epoch - 74ms/step\n",
      "153/153 [==============================] - 2s 9ms/step - loss: 0.0000e+00\n",
      "Test loss for column Mumbai_PM2.5 using ProposedModel: 0.0\n",
      "153/153 [==============================] - 4s 28ms/step\n",
      "9024/9024 [==============================] - 2114s 234ms/step\n",
      "9024/9024 [==============================] - 2198s 243ms/step\n",
      "9024/9024 [==============================] - 2279s 252ms/step\n",
      "9024/9024 [==============================] - 2342s 259ms/step\n",
      "9024/9024 [==============================] - 1846s 204ms/step\n",
      "9024/9024 [==============================] - 1643s 182ms/step\n",
      "9024/9024 [==============================] - 1399s 155ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. None expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24108\\2072789733.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\thread.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24108\\2072789733.py\u001b[0m in \u001b[0;36mprocess_and_save_results\u001b[1;34m(model_name, column, df, history, y_test_inverse, y_pred_inverse, model, X_train, feature_transformed)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mforecast_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeseriesGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mpredicted_values_forecast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforecast_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0mpredicted_values_forecast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_values_forecast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_values_forecast\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_forecast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         X = check_array(\n\u001b[0m\u001b[0;32m    566\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             )\n\u001b[0;32m   1042\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1044\u001b[0m                 \u001b[1;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m                 \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Add, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pywt\n",
    "import tensorflow as tf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Enable GPU usage\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# Find CSV files\n",
    "files = glob.glob(r'D:\\A_NAUSHAD\\E\\Dataset\\Book6.csv')\n",
    "\n",
    "def wavelet_transform(data, wavelet='db1'):\n",
    "    coeffs = pywt.wavedec(data, wavelet, mode='periodization')\n",
    "    return coeffs\n",
    "\n",
    "def inverse_wavelet_transform(coeffs, wavelet='db1'):\n",
    "    return pywt.waverec(coeffs, wavelet, mode='periodization')\n",
    "\n",
    "# Function to evaluate predictions\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mslr = tf.keras.metrics.mean_squared_logarithmic_error(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae.numpy().mean(),\n",
    "        \"mse\": mse.numpy().mean(),\n",
    "        \"rmse\": rmse.numpy().mean(),\n",
    "        \"mape\": mape.numpy().mean(),\n",
    "        \"mslr\": mslr.numpy().mean(),\n",
    "    }\n",
    "\n",
    "# Proposed unique model with residual connections\n",
    "def create_proposed_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=input_shape))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "\n",
    "    # Adding residual connections\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    residual_1 = model.layers[-1].output\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    residual_2 = model.layers[-1].output\n",
    "\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    residual_3 = model.layers[-1].output\n",
    "\n",
    "    # Summing up the residuals\n",
    "    residual_sum = Add()([residual_1, residual_2, residual_3])\n",
    "    \n",
    "    model.add(Dense(1))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Function to save DataFrame to CSV\n",
    "def save_to_csv(df, file_path):\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "# Function to process and save results\n",
    "def process_and_save_results(model_name, column, df, history, y_test_inverse, y_pred_inverse, model, X_train, feature_transformed):\n",
    "    forecast_dates = pd.date_range(start='2023-01-28', end='2024-12-31', freq='H')\n",
    "    df_forecast = pd.DataFrame(index=forecast_dates)\n",
    "    \n",
    "    feature_scaled = feature_transformed.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_scaled = scaler.fit_transform(feature_scaled)\n",
    "\n",
    "    forecast_generator = TimeseriesGenerator(feature_scaled, np.zeros(len(feature_scaled)), length=len(X_train), sampling_rate=1, batch_size=1)\n",
    "    predicted_values_forecast = model.predict(forecast_generator)\n",
    "    predicted_values_forecast = scaler.inverse_transform(predicted_values_forecast)\n",
    "\n",
    "    if len(predicted_values_forecast) > len(df_forecast):\n",
    "        predicted_values_forecast = predicted_values_forecast[:len(df_forecast)]\n",
    "    else:\n",
    "        forecast_values = np.full((len(df_forecast), 1), np.nan)\n",
    "        forecast_values[:len(predicted_values_forecast)] = predicted_values_forecast\n",
    "        predicted_values_forecast = forecast_values\n",
    "\n",
    "    df_forecast[column] = predicted_values_forecast\n",
    "    df_forecast.to_csv(f'D:/A_NAUSHAD/E/RESULTS/FORE/{model_name}_{column}_Wave_fore.csv')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_forecast.index, df_forecast[column], label='Forecasted')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.title(f'Forecast of Hourly {column} concentration using {model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    pd.DataFrame(history.history['loss']).to_csv(f'D:/A_NAUSHAD/E/RESULTS/LOSS/{model_name}_{column}_Wave_loss.csv')\n",
    "    pd.DataFrame(history.history['val_loss']).to_csv(f'D:/A_NAUSHAD/E/RESULTS/LOSS/{model_name}_{column}_Wave_val_loss.csv')\n",
    "\n",
    "    predictions_train = model.predict(X_train)\n",
    "    pd.DataFrame(predictions_train).to_csv(f'D:/A_NAUSHAD/E/RESULTS/PRED/{model_name}_{column}_Wave_train_pred.csv')\n",
    "    predictions_test = model.predict(X_test)\n",
    "    pd.DataFrame(predictions_test).to_csv(f'D:/A_NAUSHAD/E/RESULTS/PRED/{model_name}_{column}_Wave_test_pred.csv')\n",
    "\n",
    "    eval_results = evaluate_preds(y_true=y_test_inverse, y_pred=y_pred_inverse)\n",
    "    eval_df = pd.DataFrame.from_dict(eval_results, orient='index', columns=['value'])\n",
    "    eval_df.to_csv(f'D:/A_NAUSHAD/E/RESULTS/EVAL/{model_name}_{column}_Wave_eval.csv')\n",
    "\n",
    "# Dictionary of models\n",
    "models = {\n",
    "    \"ProposedModel\": create_proposed_model  # Added the proposed model here\n",
    "}\n",
    "\n",
    "# Loop over files and models\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, parse_dates=['Date'], index_col=['Date'])\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        tasks = []\n",
    "\n",
    "        for column in df.columns:\n",
    "            feature = df[[column]].values\n",
    "            target = df[[column]].values\n",
    "\n",
    "            feature_wavelet = wavelet_transform(feature)\n",
    "            target_wavelet = wavelet_transform(target)\n",
    "\n",
    "            # Reshape wavelet coefficients to 2D arrays\n",
    "            feature_transformed = feature_wavelet[0].reshape(-1, 1)\n",
    "            target_transformed = target_wavelet[0].reshape(-1, 1)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(feature_transformed, target_transformed, test_size=0.15, random_state=1, shuffle=False)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=1, shuffle=False)\n",
    "\n",
    "            # Ensure the input to Dense is 2D\n",
    "            X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "            X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "            X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "            # Debug print to check the shapes of the data\n",
    "            print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
    "            print(f'X_val shape: {X_val.shape}, y_val shape: {y_val.shape}')\n",
    "            print(f'X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "            for model_name, create_model in models.items():\n",
    "                model = create_model((X_train.shape[1],))\n",
    "                \n",
    "                lr_monitor = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=2, factor=0.5, cooldown=1)\n",
    "                early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    X_train, y_train, \n",
    "                    epochs=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=128,\n",
    "                    callbacks=[lr_monitor, early_stopping],\n",
    "                    verbose=2\n",
    "                )\n",
    "                \n",
    "                loss = model.evaluate(X_test)\n",
    "                print(f'Test loss for column {column} using {model_name}: {loss}')\n",
    "                \n",
    "#                 plt.plot(model.history[\"loss\"],label=\"loss\")\n",
    "#                 plt.plot(model.history[\"val_loss\"],label=\"val_loss\")\n",
    "#                 plt.legend(loc=\"best\")\n",
    "#                 plt.xlabel(\"No. Of Epochs\")\n",
    "#                 plt.ylabel(\"mse score\")\n",
    "                \n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                y_test_wavelet = list(target_wavelet)\n",
    "                y_test_wavelet[0] = y_test.flatten()\n",
    "                y_pred_wavelet = list(target_wavelet)\n",
    "                y_pred_wavelet[0] = y_pred.flatten()\n",
    "\n",
    "                y_test_inverse = inverse_wavelet_transform(y_test_wavelet)\n",
    "                y_pred_inverse = inverse_wavelet_transform(y_pred_wavelet)\n",
    "\n",
    "                tasks.append(executor.submit(process_and_save_results, model_name, column, df, history, y_test_inverse, y_pred_inverse, model, X_train, feature_transformed))\n",
    "\n",
    "        for task in tasks:\n",
    "            task.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac278f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
