{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246b1570-96f4-45ab-a885-15849e43d473",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bigdl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbigdl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchronos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTSTrainer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbigdl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchronos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TCN, LSTM, NBeats\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbigdl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchronos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSDataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bigdl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from bigdl.chronos.autots import AutoTSTrainer\n",
    "from bigdl.chronos.autots.model import TCN, LSTM, NBeats\n",
    "from bigdl.chronos.data import TSDataset\n",
    "import os\n",
    "\n",
    "# Load and preprocess the air pollution data\n",
    "def load_and_preprocess_data(file_path, target_features):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Correct the date format\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%d-%m-%Y %H:%M', dayfirst=True, errors='coerce')\n",
    "\n",
    "    # Drop rows with invalid or missing date values\n",
    "    data = data.dropna(subset=['Date'])\n",
    "\n",
    "    # Set the 'Date' column as the index\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "    # Ensure the target features exist\n",
    "    for target_feature in target_features:\n",
    "        if target_feature not in data.columns:\n",
    "            raise ValueError(f\"Target column '{target_feature}' not found in the dataset.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Define your features and target\n",
    "target_features = ['PM2.5', 'PM10', 'RH', 'SR']\n",
    "seq_len = 12  # You can adjust the sequence length here\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"E:/Q/Q_DATA/pm_sr.csv\"\n",
    "df = load_and_preprocess_data(file_path, target_features)\n",
    "\n",
    "# Store the date for the test set\n",
    "dates = df.index\n",
    "\n",
    "# Scale the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
    "\n",
    "# Create Chronos TSDataset\n",
    "tsdata = TSDataset.from_pandas(df=scaled_df, dt_col='Date', target_col=target_features, \n",
    "                               feature_col=scaled_df.columns.tolist(), \n",
    "                               lookback=seq_len, horizon=1)\n",
    "\n",
    "# Train-Test Split\n",
    "train_tsdata, test_tsdata = tsdata.split(train_ratio=0.8)\n",
    "\n",
    "# Get the test set dates after splitting\n",
    "test_dates = dates[-len(test_tsdata):]\n",
    "\n",
    "# Initialize AutoTSTrainer with LSTM as an example (can switch to TCN, NBeats, etc.)\n",
    "trainer = AutoTSTrainer(model=LSTM(input_feature_num=len(target_features),\n",
    "                                  output_feature_num=len(target_features),\n",
    "                                  past_seq_len=seq_len, \n",
    "                                  hidden_dim=64,\n",
    "                                  layer_num=2, \n",
    "                                  dropout=0.2),\n",
    "                        search_alg=\"random\",  # or use \"bayesian\"\n",
    "                        optimization_metric=\"mse\",  # Optimization metric for the best model\n",
    "                        loss=\"mse\", \n",
    "                        logs_dir=\"./chronos_logs\",\n",
    "                        cpus_per_trial=4)\n",
    "\n",
    "# Train the model\n",
    "best_model = trainer.fit(train_tsdata, val_ratio=0.1, epochs=5)\n",
    "\n",
    "# Test the model on the test dataset\n",
    "y_pred = best_model.predict(test_tsdata)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, len(target_features)))\n",
    "\n",
    "# Evaluate the model\n",
    "mse = ((y_pred_rescaled - test_tsdata.get_target().reshape(-1, len(target_features))) ** 2).mean()\n",
    "print(f\"Mean Squared Error on test set: {mse}\")\n",
    "\n",
    "# Prepare the actual and predicted values along with the dates\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': test_dates,  # Add the test set dates\n",
    "    'Predicted PM2.5': y_pred_rescaled[:, 0],\n",
    "    'Predicted PM10': y_pred_rescaled[:, 1],\n",
    "    'Predicted RH': y_pred_rescaled[:, 2],\n",
    "    'Predicted SR': y_pred_rescaled[:, 3],\n",
    "    'Actual PM2.5': test_tsdata.get_target().reshape(-1, len(target_features))[:, 0],\n",
    "    'Actual PM10': test_tsdata.get_target().reshape(-1, len(target_features))[:, 1],\n",
    "    'Actual RH': test_tsdata.get_target().reshape(-1, len(target_features))[:, 2],\n",
    "    'Actual SR': test_tsdata.get_target().reshape(-1, len(target_features))[:, 3]\n",
    "})\n",
    "\n",
    "# Save results to CSV\n",
    "results_csv_path = os.path.join(\"E:/Q/RESULTS\", \"predictions_vs_actuals_with_dates.csv\")\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Predictions and actuals saved to {results_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacdaf1-f24d-4f85-ad27-86e1b9a16517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
